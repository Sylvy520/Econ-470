---
title: "Section 4: Demand for Cigarettes and Instrumental Variables"
subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1050px></html>"
author: Ian McCarthy | Emory University
date: Econ 470 & HLTH 470 #"`r format(Sys.time(), '%d %B %Y')`"
header-includes: 
  - \usepackage{graphicx}
  - \usepackage{amsmath}
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, custom.css] 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "macros.js"
---

<!-- Adjust some CSS code for font size and maintain R code font size -->
<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 2em 1em 2em;    
}
.remark-code {
  font-size: 15px;
}
.remark-inline-code { 
    font-size: 20px;
}
</style>


<!-- Set R options for how code chunks are displayed and load packages -->
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(
  fig.align="center",  
  fig.height=3, #fig.width=6,
  # out.width="748px", #out.length="520.75px",
  dpi=300, #fig.path='Figs/',
  cache=T,# echo=F, warning=F, message=F
  warning = FALSE, 
  message = FALSE, 
  cache.lazy = FALSE,
  error=TRUE
  )

knitr::opts_hooks$set(fig.callout = function(options) {
  if(options$fig.callout) {
    options$echo = FALSE
  }
  options
})

if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, ggplot2, dplyr, lubridate, readr, readxl, hrbrthemes,
               scales, plotly, gganimate, cobalt, ivpack, stargazer)
```

# Table of contents

1. [Cigarettes and Prices](#smoking_lit)

2. [Instrumental Variables](#IV)

3. [Cigarette Data](#smoking_data)

4. [Estimated Demand for Cigarettes](#smoking_demand)



<!-- New Section -->
---
class: inverse, center, middle
name: smoking_lit

# Background on Cigarettes and Pricing

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>


---
# History of Smoking

<br>
.center[
  ![:scale 700px](https://media.giphy.com/media/TdQfYM6KpYQUM/giphy.gif)
]

---
# History of Smoking
- Widespread smoking begin in late 1800s
- Lung cancer becoming more common after 1930s
- First evidence of link in 1950s
- Surgeon general's report in 1964


---
# Why it matters
1. Extreme public health concerns
  - Lung cancer prevalence
  - Fetal and baby health
2. Economic questions
  - Is it an information problem?
  - Externalities (second-hand smoke)
  - Moral hazard due to insurance
  
---
# In our case
We want to focus on estimating demand for cigarettes. By this, I mean estimating price elasticity of demand.

--
<br>

We'll show that standard OLS isn't going to do this very well.

<!-- New Section -->
---
class: inverse, center, middle
name: IV

# Instrumental Variables

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>


---
# What is instrumental variables
Instrumental Variables (IV) is a way to identify causal effects using variation in treatment particpation that is due to an *exogenous* variable that is only related to the outcome through treatment.


---
# Why bother with IV?
Two reasons to consider IV:
1. Selection on unobservables
2. Reverse causation

--
<br>

Either problem is sometimes loosely referred to as *endogeneity*

---
# Simple example
- $y = \beta x + \varepsilon (x)$,<br>
where $\varepsilon(x)$ reflects the dependence between our observed variable and the error term.<br>

- Simple OLS will yield<br>
$\frac{dy}{dx} = \beta + \frac{d\varepsilon}{dx} \neq \beta$


---
# What does IV do?
- The regression we want to do: <br>
$y_{i} = \alpha + \delta W_{i} + \gamma A_{i} + \epsilon_{i}$,<br>
where $W_{i}$ is treatment (think of schooling for now) and $A_{i}$ is something like ability.

- $A_{i}$ is unobserved, so instead we run: <br>
$y_{i} = \alpha + \beta W_{i} + \epsilon_{i}$

- From this "short" regression, we don't actually estimate $\delta$. Instead, we get an estimate of<br>
$\beta = \delta + \lambda_{ws}\gamma \neq \delta$,<br>
where $\lambda_{ws}$ is the coefficient of a regression of $A_{i}$ on $W_{i}$. 

---
# Intuition
IV will recover the "long" regression without observing underlying ability<br>

--
<br>

*IF* our IV satisfies all of the necessary assumptions.

---
# More formally
- We want to estimate<br>
$E[Y_{i} | W_{i}=1] - E[Y_{i} | W_{i}=0]$

- With instrument $Z_{i}$ that satisfies relevant assumptions, we can estimate this as<br>
$E[Y_{i} | W_{i}=1] - E[Y_{i} | W_{i}=0] = \frac{E[Y_{i} | Z_{i}=1] - E[Y_{i} | Z_{i}=0]}{E[W_{i} | Z_{i}=1] - E[W_{i} | Z_{i}=0]}$

- In words, this is effect of the instrument on the outcome ("reduced form") divded by the effect of the instrument on treatment ("first stage")


---
# IVs in practice
Easy to think of in terms of randomized controlled trial...

--
<br>

 Measure    | Offered Seat | Not Offered Seat | Difference 
 ---------- | ------------ | ---------------- | ---------- 
 Score      | -0.003       | -0.358           | 0.355      
 % Enrolled | 0.787        | 0.046            | 0.741   
 Effect     |              |                  | 0.48

<br>

.footnote[
Angrist *et al.*, 2012. "Who Benefits from KIPP?" *Journal of Policy Analysis and Management*.
] 


---
# What is IV *really* doing
Think of IV as two-steps:

1. Isolate variation due to the instrument only (not due to endogenous stuff)
2. Estimate effect on outcome using only this source of variation

---
# In regression terms
Interested in estimating $\delta$ from $y_{i} = \alpha + \beta x_{i} + \delta W_{i} + \varepsilon_{i}$, but $W_{i}$ is endogenous (no pure "selection on observables").

--
<br>

Step 1: With instrument $Z_{i}$, we can regress $W_{i}$ on $Z_{i}$ and $x_{i}$,<br>
$W_{i} = \lambda + \theta Z_{i} + \kappa x_{i} + \nu$<br>
and form prediction $\hat{W}_{i}$.

--
<br>

Step 2: Regress $y_{i}$ on $x_{i}$ and $\hat{W}_{i}$,<br>
$y_{i} = \alpha + \beta x_{i} + \delta \hat{W}_{i} + \xi_{i}$

---
# In regression terms
But in practice, *DON'T* do this in two steps. Why?

--
<br>

Because standard errors are wrong...not accounting for noise in prediction, $\hat{W}_{i}$. The appropriate fix is built into most modern stats programs.


---
# Key IV assumptions
1. *Exclusion:* Instrument is uncorrelated with the error term<br>
2. *Validity:* Instrument is correlated with the endogenous variable<br>
3. *Monotonicity:* Treatment more (less) likely for those with higher (lower) values of the instrument<br>

--
<br>

Assumptions 1 and 2 sometimes grouped into an *only through* condition.



<!-- New Section -->
---
class: inverse, center, middle
name: smoking_data

# Cigarette Data

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>


---
# The Data
```{r eval=T, include=F}
#cig.data <- read_rds("C:/Users/immccar/CloudStation/Professional/Research Projects/_Git/CDC-Tobacco/data/TaxBurden_Data.rds")
cig.data <- read_rds("D:/CloudStation/Professional/Research Projects/_Git/CDC-Tobacco/data/TaxBurden_Data.rds")
```
- Data from [CDC Tax Burden on Tobacco](https://data.cdc.gov/Policy/The-Tax-Burden-on-Tobacco-1970-2018/7nwe-3aj9/data)
- Visit GitHub repository for other info: [Tobacco GitHub repository](https://github.com/imccart/CDC-Tobacco)
- Supplement with CPI data, also in GitHub repo.


---
# Cigarette Sales
```{r cig-sales, eval=FALSE, warning=FALSE}
cig.data %>% 
  ggplot(aes(x=Year,y=sales_per_capita)) + 
  stat_summary(fun.y="mean",geom="line") +
  labs(
    x="Year",
    y="Packs per Capita",
    title="Cigarette Sales"
  ) + theme_bw() +
  scale_x_continuous(breaks=seq(1970, 2020, 5))
```
.plot-callout[
```{r cig-sales-callout, ref.label="cig-sales", fig.callout=TRUE, warning=FALSE}
```
]

---
# Cigarette Sales

```{r cig-sales-output, ref.label="cig-sales", fig.callout=TRUE, warning=FALSE}
```

---
# Cigarette Prices
```{r cig-price, eval=FALSE, warning=FALSE}
cig.data %>% 
  ggplot(aes(x=Year,y=price_cpi)) + 
  stat_summary(fun.y="mean",geom="line") +
  labs(
    x="Year",
    y="Price per Pack ($)",
    title="Cigarette Prices in 2010 Real Dollars"
  ) + theme_bw() +
  scale_x_continuous(breaks=seq(1970, 2020, 5))
```
.plot-callout[
```{r cig-price-callout, ref.label="cig-price", fig.callout=TRUE, warning=FALSE}
```
]

---
# Cigarette Prices

```{r cig-price-output, ref.label="cig-price", fig.callout=TRUE, warning=FALSE}
```


<!-- New Section -->
---
class: inverse, center, middle
name: smoking_demand

# Estimating Demand for Cigarettes

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---
# Naive estimate
Clearly a strong relationship between prices and sales. For example, just from OLS:
```{r}
cig.data <- cig.data %>% mutate(ln_sales=log(sales_per_capita),
                                ln_price_cpi=log(price_cpi),
                                ln_price=log(cost_per_pack),
                                tax_cpi=tax_state*(index/218),
                                total_tax_cpi=tax_dollar*(index/218),                                
                                ln_state_tax=log(tax_cpi))
ols <- lm(ln_sales ~ ln_price, data=cig.data)
summary(ols)
```

---
# Is this causal?
- But is that the true demand curve?

- Aren't other things changing that tend to reduce cigarette sales?

---
# Tax as an IV
```{r cig-tax, eval=FALSE, warning=FALSE}
cig.data %>% 
  ggplot(aes(x=Year,y=total_tax_cpi)) + 
  stat_summary(fun.y="mean",geom="line") +
  labs(
    x="Year",
    y="Tax per Pack ($)",
    title="Cigarette Taxes in 2010 Real Dollars"
  ) + theme_bw() +
  scale_x_continuous(breaks=seq(1970, 2020, 5))
```

.plot-callout[
```{r cig-tax-callout, ref.label="cig-tax", fig.callout=TRUE, warning=FALSE}
```
]

---
# Tax as an IV

```{r cig-tax-output, ref.label="cig-tax", fig.callout=TRUE, warning=FALSE}
```


---
# IV Results

```{r}
ivs <- ivreg(ln_sales ~ ln_price | total_tax_cpi, 
             data=cig.data)
summary(ivs)
```

---
# Two-stage equivalence
```{r}
step1 <- lm(ln_price ~ total_tax_cpi, data=cig.data)
pricehat <- predict(step1)
step2 <- lm(ln_sales ~ pricehat, data=cig.data)
summary(step2)
```




---
# Different specifications
```{r echo=FALSE}
ols1 <- lm(ln_sales ~ ln_price_cpi, data=cig.data)
ols2 <- lm(ln_sales ~ ln_price_cpi + factor(state), data=cig.data)
ols3 <- lm(ln_sales ~ ln_price_cpi + factor(state) + factor(Year), data=cig.data)

ivs1 <- ivreg(ln_sales ~ ln_price_cpi | total_tax_cpi, data=cig.data)
ivs2 <- ivreg(ln_sales ~ ln_price_cpi + factor(state) | 
                total_tax_cpi + factor(state), data=cig.data)
ivs3 <- ivreg(ln_sales ~ ln_price_cpi + factor(state) + factor(Year) | 
                total_tax_cpi + factor(state) + factor(Year), data=cig.data)
stargazer(ols1, ols2, ols3, ivs1, ivs2, ivs3, 
          keep=c("ln_price_cpi"), 
          type="html",
          column.labels = c("OLS","IV"),
          column.separate=c(3,3),
          keep.stat=c("n"),
          model.names=FALSE,
          add.lines=list(c("State FE","No","Yes","Yes","No","Yes","Yes"),
                         c("Year FE","No","No","Yes","No","No","Yes")),
          covariate.labels = "Log Price",
          dep.var.labels = "Log Sales per Capita")
```


---
# Test the IV
```{r echo=FALSE}
first1 <- lm(ln_price_cpi ~ total_tax_cpi, data=cig.data)
first2 <- lm(ln_price_cpi ~ total_tax_cpi + factor(state), data=cig.data)
first3 <- lm(ln_price_cpi ~ total_tax_cpi + factor(state) + factor(Year), data=cig.data)

rf1 <- lm(ln_sales ~ total_tax_cpi, data=cig.data)
rf2 <- lm(ln_sales ~ total_tax_cpi + factor(state), data=cig.data)
rf3 <- lm(ln_sales ~ total_tax_cpi + factor(state) + factor(Year), data=cig.data)


stargazer(first1, first2, first3, rf1, rf2, rf3,
          keep=c("total_tax_cpi"), 
          type="html",
          column.labels = c("First Stage","Reduced Form"),
          column.separate=c(3,3),
          keep.stat=c("n"),
          model.names=FALSE,
          add.lines=list(c("State FE","No","Yes","Yes","No","Yes","Yes"),
                         c("Year FE","No","No","Yes","No","No","Yes")),
          covariate.labels = "Tax per Pack",
          dep.var.labels = "Log Price")
```

---
# Summary
1. Most elasticities of around -0.25% to -0.37%
2. Much larger elasticities when including year fixed effects
3. Perhaps not too outlandish given more recent evidence: [NBER Working Paper](https://www.nber.org/papers/w22251.pdf).
